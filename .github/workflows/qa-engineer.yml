name: AI QA Engineer

# This workflow is designed to be copied to your repository.
# It can run manually OR on pull requests.
#
# For PR testing, you need a preview deployment. Options:
# 1. Vercel/Netlify: Set PREVIEW_URL env var in your deployment workflow
# 2. workflow_call: Call this workflow from another workflow with the URL
# 3. Manual: Uncomment pull_request trigger and set PREVIEW_URL secret/var

on:
  pull_request:
    types: [opened, synchronize, reopened]
  workflow_dispatch:
    inputs:
      url:
        description: 'URL to test (required for manual runs)'
        required: false
        type: string
      focus:
        description: 'Focus area: accessibility, performance, forms, mobile, all'
        required: false
        type: string
        default: 'all'
      fail_on_bugs:
        description: 'Fail workflow if critical/high bugs found'
        required: false
        type: boolean
        default: true
  workflow_call:
    inputs:
      url:
        description: 'URL to test'
        required: true
        type: string
      focus:
        description: 'Focus area'
        required: false
        type: string
        default: 'all'
      fail_on_bugs:
        description: 'Fail on critical/high bugs'
        required: false
        type: boolean
        default: true
    secrets:
      ANTHROPIC_API_KEY:
        required: true

permissions:
  contents: read
  pull-requests: write
  issues: write

env:
  NODE_VERSION: '20'
  PLAYWRIGHT_VERSION: '1.48.0'

jobs:
  qa-test:
    runs-on: ubuntu-latest
    timeout-minutes: 12

    steps:
      - name: Determine Test URL
        id: get-url
        env:
          INPUT_URL: ${{ inputs.url }}
          PREVIEW_URL: ${{ vars.PREVIEW_URL || secrets.PREVIEW_URL || '' }}
        run: |
          # Priority: explicit input > PREVIEW_URL var/secret
          if [ -n "$INPUT_URL" ]; then
            echo "test_url=$INPUT_URL" >> $GITHUB_OUTPUT
            echo "âœ“ Using provided URL: $INPUT_URL"
          elif [ -n "$PREVIEW_URL" ]; then
            echo "test_url=$PREVIEW_URL" >> $GITHUB_OUTPUT
            echo "âœ“ Using PREVIEW_URL: $PREVIEW_URL"
          else
            echo "âŒ Error: No URL provided"
            echo ""
            echo "For manual runs: provide a URL in the workflow inputs"
            echo "For PR runs: set PREVIEW_URL as a repository variable or secret"
            echo "For Vercel/Netlify: chain this workflow after your deploy workflow"
            exit 1
          fi

      - name: Validate API Key
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          if [ -z "$ANTHROPIC_API_KEY" ]; then
            echo "âŒ Error: ANTHROPIC_API_KEY secret is not configured"
            echo "Please add your Anthropic API key to repository secrets."
            exit 1
          fi
          echo "âœ“ API key configured"

      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Cache Playwright Browsers
        id: playwright-cache
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: playwright-${{ runner.os }}-${{ env.PLAYWRIGHT_VERSION }}
          restore-keys: |
            playwright-${{ runner.os }}-

      - name: Install Dependencies
        run: npm ci
        env:
          PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD: 'true'

      - name: Install Playwright Browsers
        if: steps.playwright-cache.outputs.cache-hit != 'true'
        run: npx playwright install chromium --with-deps

      - name: Install Playwright Deps (cached)
        if: steps.playwright-cache.outputs.cache-hit == 'true'
        run: npx playwright install-deps chromium

      - name: Check URL is reachable
        id: url-check
        run: |
          echo "Checking if URL is reachable..."
          TEST_URL="${{ steps.get-url.outputs.test_url }}"

          # Try to reach the URL
          HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" --max-time 10 "$TEST_URL" || echo "000")

          if [ "$HTTP_CODE" = "000" ]; then
            echo "âš ï¸ Warning: Could not reach $TEST_URL (connection failed)"
            echo "reachable=false" >> $GITHUB_OUTPUT
          elif [ "$HTTP_CODE" -ge 400 ]; then
            echo "âš ï¸ Warning: $TEST_URL returned HTTP $HTTP_CODE"
            echo "reachable=false" >> $GITHUB_OUTPUT
          else
            echo "âœ“ URL is reachable (HTTP $HTTP_CODE)"
            echo "reachable=true" >> $GITHUB_OUTPUT
          fi

      - name: Create Screenshots Directory
        run: mkdir -p screenshots

      - name: Run AI QA Engineer
        id: qa-run
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          TEST_URL: ${{ steps.get-url.outputs.test_url }}
          TEST_FOCUS: ${{ inputs.focus || 'all' }}
        run: |
          echo "::group::QA Testing Configuration"
          echo "URL: $TEST_URL"
          echo "Focus Area: $TEST_FOCUS"
          echo "::endgroup::"

          echo "::group::Starting AI QA Engineer"
          START_TIME=$(date +%s)

          # 5 minute timeout, max 15 turns - be aggressive about speed
          timeout 300 npx @anthropic-ai/claude-code --print \
            --mcp-config .claude/mcp-config.json \
            --max-turns 15 \
            "SPEED IS CRITICAL. You have 3 minutes max.

             Test $TEST_URL:
             1. Navigate to URL
             2. Screenshot desktop view -> ./screenshots/desktop.png
             3. Screenshot mobile (375px) -> ./screenshots/mobile.png
             4. Check for console errors
             5. Click 2-3 buttons/links to verify they work
             6. Write ./qa-report.md: list any bugs found, or 'No issues found'

             DO NOT: test multiple viewports, do exhaustive testing, write long reports.
             STOP after basic checks. Speed > thoroughness." || {
            EXIT_CODE=$?
            if [ $EXIT_CODE -eq 124 ]; then
              echo "âš ï¸ QA testing timed out after 5 minutes"
            else
              echo "âš ï¸ QA testing encountered an error (exit code: $EXIT_CODE)"
            fi
          }

          END_TIME=$(date +%s)
          DURATION=$((END_TIME - START_TIME))
          echo "âœ“ QA testing completed in ${DURATION}s"
          echo "::endgroup::"
          echo "qa_duration_seconds=$DURATION" >> $GITHUB_OUTPUT

      - name: Upload Screenshots
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: qa-screenshots-${{ github.run_number }}
          path: screenshots/
          retention-days: 14
          if-no-files-found: ignore

      - name: Upload QA Report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: qa-report-${{ github.run_number }}
          path: qa-report.md
          retention-days: 14
          if-no-files-found: ignore

      - name: Summary
        if: always()
        run: |
          echo "## AI QA Engineer Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**URL Tested:** ${{ steps.get-url.outputs.test_url }}" >> $GITHUB_STEP_SUMMARY
          echo "**Focus Area:** ${{ inputs.focus || 'all' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f "qa-report.md" ]; then
            echo "### Report" >> $GITHUB_STEP_SUMMARY
            cat qa-report.md >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ No QA report generated. Check the logs for details." >> $GITHUB_STEP_SUMMARY
          fi

      - name: Comment on PR
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            let body = '## ðŸ¤– AI QA Engineer Report\n\n';
            body += `**URL Tested:** ${{ steps.get-url.outputs.test_url }}\n`;
            body += `**Focus Area:** ${{ inputs.focus || 'all' }}\n\n`;

            if (fs.existsSync('qa-report.md')) {
              const report = fs.readFileSync('qa-report.md', 'utf8');
              body += report;
            } else {
              body += 'âš ï¸ No QA report generated. Check the [workflow logs](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}) for details.\n';
            }

            body += `\n\n### ðŸ“Ž Artifacts\n`;
            body += `- [Screenshots](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})\n`;
            body += `- [Full Report](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})\n`;
            body += '\n---\n*Automated QA testing by [AI QA Engineer](https://github.com/tyler-james-bridges/ai-qa-engineer)*';

            // Find existing comment to update (avoid spam)
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('AI QA Engineer Report')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }

      - name: Check for critical bugs
        if: ${{ inputs.fail_on_bugs }}
        run: |
          if [ ! -f "qa-report.md" ]; then
            echo "No QA report found - skipping bug check"
            exit 0
          fi

          echo "Checking for critical/high severity bugs..."

          # Case-insensitive search for critical or high severity indicators
          CRITICAL_COUNT=$(grep -i -c -E '(critical|severity:\s*critical)' qa-report.md || echo "0")
          HIGH_COUNT=$(grep -i -c -E '(high|severity:\s*high)' qa-report.md || echo "0")

          echo "Critical bugs: $CRITICAL_COUNT"
          echo "High severity bugs: $HIGH_COUNT"

          if [ "$CRITICAL_COUNT" -gt 0 ]; then
            echo "::error::Found $CRITICAL_COUNT critical bug(s) - failing workflow"
            exit 1
          fi

          if [ "$HIGH_COUNT" -gt 0 ]; then
            echo "::warning::Found $HIGH_COUNT high severity bug(s) - failing workflow"
            exit 1
          fi

          echo "âœ“ No critical/high severity bugs found"
